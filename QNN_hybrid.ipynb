{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe47d004-1530-4487-b607-257448085df9",
   "metadata": {},
   "source": [
    "### IMPORTANT: this file contains generated content by Gemini 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf3cb7-a4e1-4256-99dd-60c43e30af0a",
   "metadata": {},
   "source": [
    "# QNN with Torch Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d3db5-efc4-4435-87fb-4d1341274522",
   "metadata": {},
   "source": [
    "## Regression: EstimatorQNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e683c78-a90e-4d51-9dac-4c0f8a32f8e2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8503ee-44a8-489c-8848-0b646fd5a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, RealAmplitudes\n",
    "from qiskit.primitives import Estimator, Sampler\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.utils.loss_functions import L2Loss # Qiskit ML loss, or use PyTorch's\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def41944-5926-4446-9dee-ff75b7c8f7af",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e322578b-2721-4f2a-8616-a48249dfbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fixed feature sizes\n",
    "NUM_FEATURES = 3\n",
    "NUM_QUBITS = NUM_FEATURES \n",
    "NUM_TARGETS = 1 \n",
    "\n",
    "# Quantum circuit parameters\n",
    "FEATURE_MAP_REPS = 1 # ADJUST AS NEEDED\n",
    "ANSATZ_REPS = 3 # ADJUST AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f57451-c163-4d39-9e16-285e28cf3aa8",
   "metadata": {},
   "source": [
    "### Feature Map, Ansatz, then QNN Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74db392f-1034-4049-bd46-16da1737a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 12\n",
      "Total circuit parameters in qc: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-60869141/ipykernel_2104523/3618193173.py:48: DeprecationWarning: The class ``qiskit.primitives.estimator.Estimator`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseEstimatorV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Estimator` class is `StatevectorEstimator`.\n",
      "  estimator = Estimator()\n"
     ]
    }
   ],
   "source": [
    "# a. Feature Map: Encodes NUM_FEATURES into NUM_QUBITS\n",
    "# ParameterVector for input features\n",
    "input_params = ParameterVector(\"x\", NUM_FEATURES)\n",
    "\n",
    "feature_map_template = PauliFeatureMap(\n",
    "    feature_dimension=NUM_FEATURES, # This tells the template how many input parameters it structurally needs\n",
    "    reps=FEATURE_MAP_REPS,\n",
    "    entanglement='linear'\n",
    ")\n",
    "\n",
    "# Assign the *specific* input parameters from the vector to the template's parameter slots\n",
    "# This creates a new circuit instance containing parameters ONLY from input_params (size NUM_FEATURES)\n",
    "feature_map = feature_map_template.assign_parameters(input_params)\n",
    "print(f\"Assigned feature map parameters: {feature_map.num_parameters}\")\n",
    "\n",
    "# Create a template to find out how many parameters it needs structurally\n",
    "ansatz_template = RealAmplitudes(NUM_QUBITS, reps=ANSATZ_REPS, entanglement=\"linear\")\n",
    "# ParameterVector for trainable weights - sized based on the template's structural parameters\n",
    "num_ansatz_params = ansatz_template.num_parameters # This was correctly calculated as 12\n",
    "weight_params = ParameterVector(\"θ\", num_ansatz_params)\n",
    "\n",
    "# Create the ansatz circuit instance by assigning the weight parameters to the template\n",
    "ansatz = ansatz_template.assign_parameters(weight_params)\n",
    "print(f\"Assigned ansatz parameters: {ansatz.num_parameters}\") \n",
    "\n",
    "# c. Combine into a full quantum circuit\n",
    "qc = QuantumCircuit(NUM_QUBITS)\n",
    "qc.compose(feature_map, inplace=True)\n",
    "qc.compose(ansatz, inplace=True)\n",
    "\n",
    "print(f\"Total circuit parameters in qc: {qc.num_parameters}\")\n",
    "\n",
    "\n",
    "# d. Define Observable(s)\n",
    "# For a single output, measure the expectation value of Pauli Z on the first qubit\n",
    "# The output of EstimatorQNN will be in the range [-1, 1] for Pauli observables\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * (NUM_QUBITS - 1), 1.0)])\n",
    "# If you have multiple qubits and want to combine their measurements, you can define multiple observables\n",
    "# or a more complex one. For instance, if NUM_TARGETS > 1 or you want a richer output from QNN:\n",
    "# observables = [SparsePauliOp(f\"{'I'*i}Z{'I'*(NUM_QUBITS-1-i)}\") for i in range(NUM_QUBITS)]\n",
    "# This would give NUM_QUBITS outputs from the QNN.\n",
    "\n",
    "# --- 3. EstimatorQNN ---\n",
    "# Uses Qiskit's Estimator primitive for expectation value computations\n",
    "# By default, Estimator uses a local statevector simulator.\n",
    "# For real hardware or more advanced simulation, configure the Estimator.\n",
    "estimator = Estimator()\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    estimator=estimator,\n",
    "    input_params=input_params,\n",
    "    weight_params=weight_params, # Parameters for trainable weights\n",
    "    observables=observable,      # Observable to measure\n",
    "    input_gradients=False       # Set to True if you need gradients w.r.t. inputs\n",
    ")\n",
    "\n",
    "# --- 4. TorchConnector ---\n",
    "# Wrap the QNN into a PyTorch module\n",
    "initial_weights = 0.01 * (2 * np.random.rand(qnn.num_weights) - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a013c2e-28f3-485b-9a3f-5237e5b68cfc",
   "metadata": {},
   "source": [
    "### HybridModel Class (add classical PyTorch pre-processing or customizations, right now too big/small SFE need more penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b2515b-c977-44f8-b0af-307ded0bab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, qnn_model):\n",
    "        super().__init__()\n",
    "        # Example: Add classical layers if needed\n",
    "        # self.classical_pre = nn.Linear(NUM_FEATURES, NUM_FEATURES) # If you want to pre-process features\n",
    "        self.qnn = qnn_model\n",
    "        # Example: Add classical layers after the QNN\n",
    "        # self.classical_post = nn.Linear(qnn_model.output_shape[0], NUM_TARGETS) # output_shape[0] is num_observables\n",
    "                                                                                # If qnn_model.output_shape is (1,), then it's 1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.classical_pre(x) # If using classical_pre\n",
    "        x = self.qnn(x)\n",
    "        # x = self.classical_post(x) # If using classical_post\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a718d-f42b-4ca6-9015-72048e4468fa",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3cfc6cc-5f61-4580-91e3-bf6017885b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_k_fold(X, y, train_indices, test_indices):\n",
    "    # Separate train/test split\n",
    "    X_train_raw, X_test_raw = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # Separate element column from the actual features\n",
    "    element_test = X_test_raw[:, 0]\n",
    "    \n",
    "    # Drop the element column (first column)\n",
    "    X_train = X_train_raw[:, 1:]\n",
    "    X_test = X_test_raw[:, 1:]\n",
    "\n",
    "    full_X = np.vstack([X_train, X_test])\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(full_X)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test, element_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8a4ec-641d-4b08-9b42-3bf32cb1951d",
   "metadata": {},
   "source": [
    "### Custom Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddebd4e-1171-403e-b447-85a913e62db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, small_threshold=0.1, large_threshold=10.0, weight_small=2.0, weight_large=2.0):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        \n",
    "        # Thresholds to define small and large values\n",
    "        self.small_threshold = small_threshold\n",
    "        self.large_threshold = large_threshold\n",
    "        \n",
    "        # Weights for small and large targets\n",
    "        self.weight_small = weight_small\n",
    "        self.weight_large = weight_large\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate absolute error between predicted and true values\n",
    "        error = torch.abs(y_pred - y_true)\n",
    "        \n",
    "        # Define weights based on the thresholds\n",
    "        weights = torch.ones_like(y_true)  # Start with 1.0 (no extra weighting)\n",
    "        \n",
    "        # Apply higher weight for small targets\n",
    "        weights[y_true < self.small_threshold] = self.weight_small\n",
    "        \n",
    "        # Apply higher weight for large targets\n",
    "        weights[y_true > self.large_threshold] = self.weight_large\n",
    "        \n",
    "        # Calculate weighted loss\n",
    "        weighted_error = weights * error\n",
    "        \n",
    "        # Return the mean of the weighted error\n",
    "        return torch.mean(weighted_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9612c-1bac-49fa-9a67-f8f1bb4a958e",
   "metadata": {},
   "source": [
    "### Main(): Data Loading, Training, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a3e210-bd7e-435e-ad5c-05a959182d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_name, date):\n",
    "\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "    dataset_name = \"qml_training-validation-data.csv\"\n",
    "    df = pd.read_csv(dataset_name)\n",
    "    X = df[['Element', 'el_neg', 'B/GPa', 'Volume/A^3']].values\n",
    "    y = df['SFE/mJm^-3'].values\n",
    "\n",
    "    y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    y = y_scaler.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=X.shape[0]//3, n_repeats=3)\n",
    "\n",
    "    df = pd.DataFrame(columns=['element test', 'actual', 'predicted'])\n",
    "\n",
    "    LEARNING_RATE = 0.01\n",
    "    BATCH_SIZE = 30\n",
    "    NUM_EPOCHS = 50 \n",
    "    # LOSS = nn.MSELoss()\n",
    "    LOSS = nn.HuberLoss() # maybe less prone to extreme values\n",
    "    # LOSS = nn.SmoothL1Loss()\n",
    "    # LOSS = WeightedLoss(small_threshold=5, large_threshold=30, weight_small=16.0, weight_large=4.0)\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    print(\"\\n--- Start K-Fold Loop ---\")\n",
    "\n",
    "    for train_indices, test_indices in rkf.split(X):\n",
    "        qnn_torch_model = TorchConnector(qnn, initial_weights=torch.tensor(initial_weights, dtype=torch.float32))\n",
    "        # model = qnn_torch_model # for purely quantum nn\n",
    "        model = HybridModel(qnn_torch_model) # classical modifications in the HybridQNN class\n",
    "        \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, y_train, X_test, y_test, element_test = prepare_dataset_k_fold(X, y, train_indices, test_indices)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(f\"Training data shape: X_train_t: {X_train_t.shape}, y_train_t: {y_train_t.shape}\")\n",
    "        print(f\"Testing data shape: X_test_t: {X_test_t.shape}, y_test_t: {y_test_t.shape}\")\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        print(f\"\\n--- Starting Training {i}th---\")\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()    # Clear gradients\n",
    "                outputs = model(batch_X) # Forward pass\n",
    "                loss = LOSS(outputs, batch_y) # Calculate loss\n",
    "                loss.backward()          # Backward pass (compute gradients)\n",
    "                optimizer.step()         # Update weights\n",
    "                running_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            # Validation/Test phase\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculations\n",
    "                for batch_X_test, batch_y_test in test_loader:\n",
    "                    outputs_test = model(batch_X_test)\n",
    "                    loss_test = LOSS(outputs_test, batch_y_test)\n",
    "                    test_loss += loss_test.item() * batch_X_test.size(0)\n",
    "        \n",
    "            epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "            test_losses.append(epoch_test_loss)\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}\")\n",
    "        \n",
    "        print(\"--- Training Finished ---\")\n",
    "        \n",
    "        # --- 9. Plotting Training History (Optional) ---\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title('Training and Test Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # plt.show()\n",
    "        plt.savefig(f\"QNN/regression/figure/{date}/{date}_training_history_#{i}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # --- 10. Evaluation on Test Set (Example) ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_test, batch_y_test in test_loader:\n",
    "                outputs_test = model(batch_X_test)\n",
    "                all_preds.extend(outputs_test.cpu().numpy())\n",
    "                all_targets.extend(batch_y_test.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_preds = y_scaler.inverse_transform(all_preds.reshape(-1,1))\n",
    "        all_targets = y_scaler.inverse_transform(all_targets.reshape(-1,1))\n",
    "        \n",
    "        new_row = {'element test': element_test,\n",
    "                   'actual': np.array(all_targets).flatten(),\n",
    "                   'predicted': np.array(all_preds).flatten()}\n",
    "        df.loc[len(df)] = new_row\n",
    "        \n",
    "        # Example: Scatter plot for regression\n",
    "        if NUM_TARGETS == 1: # Simple plot if single target variable\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.scatter(all_targets, all_preds, alpha=0.5)\n",
    "            plt.plot([min(all_targets.min(), all_preds.min()), max(all_targets.max(), all_preds.max())],\n",
    "                     [min(all_targets.min(), all_preds.min()), max(all_targets.max(), all_preds.max())],\n",
    "                     'k--', lw=2, label='Ideal')\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title('Actual vs. Predicted Values on Test Set')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            # plt.show()\n",
    "            plt.savefig(f\"QNN/regression/figure/{date}/{date}_test_result_#{i}.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # Further evaluation metrics can be added here (e.g., R-squared for regression, accuracy for classification)\n",
    "        # final_mse = mean_squared_error(all_targets, all_preds)\n",
    "        # final_r2 = r2_score(all_targets, all_preds)\n",
    "        # print(f\"\\n--- Final Test Set Evaluation ---\")\n",
    "        # print(f\"Mean Squared Error (MSE): {final_mse:.4f}\")\n",
    "        # print(f\"R-squared (R2 Score): {final_r2:.4f}\")\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    df.at[0, \"info\"] = [f\"DATASET: {dataset_name}, FEATURE_MAP_REPS = {FEATURE_MAP_REPS}, ANSATZ_REPS = {ANSATZ_REPS}, LEARNING_RATE = {LEARNING_RATE}, BATCH_SIZE = {BATCH_SIZE}, NUM_EPOCHS = {NUM_EPOCHS}, LOSS: {LOSS}\"]\n",
    "    df.to_csv(file_name, index=False) \n",
    "\n",
    "# To make predictions on new data:\n",
    "# new_data_np = np.array([[val1, val2, val3], ...]) # Your new data\n",
    "# new_data_scaled = scaler_X.transform(new_data_np) # Don't forget to scale\n",
    "# new_data_t = torch.tensor(new_data_scaled, dtype=torch.float32)\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(new_data_t)\n",
    "# print(f\"Predictions for new data: {predictions.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab2765-ad63-4947-825a-6c06df7745b0",
   "metadata": {},
   "source": [
    "### Run Main() to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22246ca-78cd-4e2c-b86b-120d15d90dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '05_20_25_0'\n",
    "file_name = f'QNN/regression/result/{date}.csv'\n",
    "!mkdir QNN/regression/figure/$date\n",
    "\n",
    "main(file_name, date)\n",
    "\n",
    "# 14_0 ansatz reps=2, _1 reps=3\n",
    "# 15_0 HuberLoss, 15_1 SmoothL1Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182fe2c-e519-4352-b051-13191d0715f8",
   "metadata": {},
   "source": [
    "## Classification： "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffeaa62-d1c8-43d8-b8e1-e376669a0530",
   "metadata": {},
   "source": [
    "### Feature Map, Ansatz, then QNN Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ce65ae-d12a-4a94-89cf-ab8a26b07c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned feature map parameters: 3\n",
      "Assigned ansatz parameters: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-60869141/ipykernel_2104523/844644645.py:33: DeprecationWarning: The class ``qiskit.primitives.sampler.Sampler`` is deprecated as of qiskit 1.2. It will be removed no earlier than 3 months after the release date. All implementations of the `BaseSamplerV1` interface have been deprecated in favor of their V2 counterparts. The V2 alternative for the `Sampler` class is `StatevectorSampler`.\n",
      "  sampler = Sampler()\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "input_params = ParameterVector(\"x\", NUM_FEATURES)\n",
    "\n",
    "feature_map_template = PauliFeatureMap(\n",
    "    feature_dimension=NUM_FEATURES, # This tells the template how many input parameters it structurally needs\n",
    "    reps=FEATURE_MAP_REPS,\n",
    "    entanglement='linear'\n",
    ")\n",
    "\n",
    "# Assign the *specific* input parameters from the vector to the template's parameter slots\n",
    "# This creates a new circuit instance containing parameters ONLY from input_params (size NUM_FEATURES)\n",
    "feature_map = feature_map_template.assign_parameters(input_params)\n",
    "print(f\"Assigned feature map parameters: {feature_map.num_parameters}\")\n",
    "\n",
    "# Create a template to find out how many parameters it needs structurally\n",
    "ansatz_template = RealAmplitudes(NUM_QUBITS, reps=ANSATZ_REPS, entanglement=\"linear\")\n",
    "# ParameterVector for trainable weights - sized based on the template's structural parameters\n",
    "num_ansatz_params = ansatz_template.num_parameters # This was correctly calculated as 12\n",
    "weight_params = ParameterVector(\"θ\", num_ansatz_params)\n",
    "\n",
    "# Create the ansatz circuit instance by assigning the weight parameters to the template\n",
    "ansatz = ansatz_template.assign_parameters(weight_params)\n",
    "print(f\"Assigned ansatz parameters: {ansatz.num_parameters}\") \n",
    "\n",
    "qc = QNNCircuit(\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz_template,\n",
    ")\n",
    "\n",
    "# example 5.2 from Qiskit guide on binary classification\n",
    "parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
    "output_shape = 2  # parity = 0, 1\n",
    "\n",
    "sampler = Sampler()\n",
    "\n",
    "qnn = SamplerQNN(\n",
    "    circuit=qc,\n",
    "    interpret=parity,\n",
    "    output_shape=output_shape,\n",
    "    sampler=sampler,\n",
    "    sparse=False,\n",
    "    input_gradients=False,\n",
    ")\n",
    "\n",
    "initial_weights = 0.01 * (2 * np.random.rand(qnn.num_weights) - 1)\n",
    "qnn_torch_model = TorchConnector(qnn, initial_weights=torch.tensor(initial_weights, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd7b29-82d6-44bf-943f-0dcb56c1b27f",
   "metadata": {},
   "source": [
    "### Process the Target Bariable for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c29f42-2f0f-4a2b-9c00-6e56a7fb69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(y):\n",
    "    for i in range(0,len(y)): \n",
    "        if y[i]>19: y[i]=0\n",
    "        else: y[i]=1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429860f-c261-4088-8b3b-83fc3c808eda",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7f5484-a096-41d8-9afe-98f2f0decabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_name, date):\n",
    "\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "    dataset_name = \"qml_training-validation-data.csv\"\n",
    "    df = pd.read_csv(dataset_name)\n",
    "    X = df[['Element', 'el_neg', 'B/GPa', 'Volume/A^3']].values\n",
    "    y = df['SFE/mJm^-3'].values\n",
    "\n",
    "    y = process(y)\n",
    "    \n",
    "    # y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # y = y_scaler.fit_transform(y.reshape(-1,1))\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=X.shape[0]//3, n_repeats=3)\n",
    "\n",
    "    df = pd.DataFrame(columns=['element test', 'actual', 'predicted'])\n",
    "\n",
    "    LEARNING_RATE = 0.01\n",
    "    BATCH_SIZE = 30\n",
    "    NUM_EPOCHS = 50 \n",
    "\n",
    "    LOSS = nn.CrossEntropyLoss() # use torch.long\n",
    "    # LOSS = nn.MSELoss() # haven't tried\n",
    "    # LOSS = nn.BCELoss() # use torch.float\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    print(\"\\n--- Start K-Fold Loop ---\")\n",
    "\n",
    "    for train_indices, test_indices in rkf.split(X):\n",
    "        model = HybridModel(qnn_torch_model) \n",
    "        \n",
    "        X_train, y_train, X_test, y_test, element_test = prepare_dataset_k_fold(X, y, train_indices, test_indices)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long) # may need to change this\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)# and this if using a different Loss function. BCELoss should use float and CrossEntropy should use be long\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        print(f\"Training data shape: X_train_t: {X_train_t.shape}, y_train_t: {y_train_t.shape}\")\n",
    "        print(f\"Testing data shape: X_test_t: {X_test_t.shape}, y_test_t: {y_test_t.shape}\")\n",
    "    \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        print(f\"\\n--- Starting Training {i}th---\")\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad(set_to_none=True)    # Clear gradients\n",
    "                outputs = model(batch_X) # Forward pass\n",
    "\n",
    "                \n",
    "                \n",
    "                # print(outputs)\n",
    "                # print(batch_y)\n",
    "                loss = LOSS(outputs, batch_y) # Calculate loss\n",
    "                loss.backward()          # Backward pass (compute gradients)\n",
    "                optimizer.step()         # Update weights\n",
    "                running_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            # Validation/Test phase\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculations\n",
    "                for batch_X_test, batch_y_test in test_loader:\n",
    "                    outputs_test = model(batch_X_test)\n",
    "                    loss_test = LOSS(outputs_test, batch_y_test)\n",
    "                    test_loss += loss_test.item() * batch_X_test.size(0)\n",
    "        \n",
    "            epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "            test_losses.append(epoch_test_loss)\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {epoch_loss:.4f}, Test Loss: {epoch_test_loss:.4f}\")\n",
    "        \n",
    "        print(\"--- Training Finished ---\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title('Training and Test Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        # plt.show()\n",
    "        plt.savefig(f\"QNN/classification/figure/{date}/{date}_training_history_#{i}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_test, batch_y_test in test_loader:\n",
    "                outputs_test = model(batch_X_test)\n",
    "                all_preds.extend(outputs_test.cpu().numpy())\n",
    "                all_targets.extend(batch_y_test.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_preds_temp = []\n",
    "        for item in all_preds:\n",
    "            if(item[1] > item[0]):\n",
    "                all_preds_temp.append(1)\n",
    "            else:\n",
    "                all_preds_temp.append(0)\n",
    "        print(all_preds)\n",
    "        all_preds = np.array(all_preds_temp)\n",
    "        \n",
    "        new_row = {'element test': element_test,\n",
    "                   'actual': np.array(all_targets).flatten(),\n",
    "                   'predicted': np.array(all_preds).flatten()}\n",
    "        df.loc[len(df)] = new_row\n",
    "        \n",
    "        if NUM_TARGETS == 1:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.scatter(all_targets, all_preds, alpha=0.5)\n",
    "            plt.plot([0, 0, 1, 1],\n",
    "                     [0, 0, 1, 1],\n",
    "                     'k--', lw=2, label='Ideal')\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title('Actual vs. Predicted Values on Test Set')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            # plt.show()\n",
    "            plt.savefig(f\"QNN/classification/figure/{date}/{date}_test_result_#{i}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    df.at[0, \"info\"] = [f\"DATASET: {dataset_name}, FEATURE_MAP_REPS = {FEATURE_MAP_REPS}, ANSATZ_REPS = {ANSATZ_REPS}, LEARNING_RATE = {LEARNING_RATE}, BATCH_SIZE = {BATCH_SIZE}, NUM_EPOCHS = {NUM_EPOCHS}, LOSS: {LOSS}\"]\n",
    "    df.to_csv(file_name, index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a09e71-4f52-4770-8e25-db4f6a1e275c",
   "metadata": {},
   "source": [
    "### run main() to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a1f5c1-7079-48a7-9767-3b37387fac08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'QNN/classification/figure/05_19_25_1': File exists\n",
      "\n",
      "--- Loading and Preprocessing Data ---\n",
      "\n",
      "--- Start K-Fold Loop ---\n",
      "Training data shape: X_train_t: torch.Size([18, 3]), y_train_t: torch.Size([18])\n",
      "Testing data shape: X_test_t: torch.Size([3, 3]), y_test_t: torch.Size([3])\n",
      "\n",
      "--- Starting Training 0th---\n",
      "Epoch 1/50, Train Loss: 0.6921, Test Loss: 0.6942\n",
      "Epoch 2/50, Train Loss: 0.6890, Test Loss: 0.6946\n",
      "Epoch 3/50, Train Loss: 0.6857, Test Loss: 0.6951\n",
      "Epoch 4/50, Train Loss: 0.6823, Test Loss: 0.6954\n",
      "Epoch 5/50, Train Loss: 0.6788, Test Loss: 0.6957\n",
      "Epoch 6/50, Train Loss: 0.6753, Test Loss: 0.6959\n",
      "Epoch 7/50, Train Loss: 0.6716, Test Loss: 0.6960\n",
      "Epoch 8/50, Train Loss: 0.6679, Test Loss: 0.6961\n",
      "Epoch 9/50, Train Loss: 0.6641, Test Loss: 0.6961\n",
      "Epoch 10/50, Train Loss: 0.6602, Test Loss: 0.6961\n",
      "Epoch 11/50, Train Loss: 0.6563, Test Loss: 0.6960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQNN/classification/result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir QNN/classification/figure/$date\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(file_name, date)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# print(batch_y)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m LOSS(outputs, batch_y) \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# Backward pass (compute gradients)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()         \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     72\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/qiskit_machine_learning/connectors/torch_connector.py:169\u001b[0m, in \u001b[0;36mTorchConnector._TorchNNFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     grad_output \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# evaluate QNN gradient\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m input_grad, weights_grad \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msparse:\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/qiskit_machine_learning/neural_networks/neural_network.py:256\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m    254\u001b[0m input_, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(input_data)\n\u001b[1;32m    255\u001b[0m weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_weights(weights)\n\u001b[0;32m--> 256\u001b[0m input_grad, weight_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m input_grad_reshaped, weight_grad_reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_backward_output(\n\u001b[1;32m    259\u001b[0m     input_grad, weight_grad, shape\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_grad_reshaped, weight_grad_reshaped\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/qiskit_machine_learning/neural_networks/sampler_qnn.py:423\u001b[0m, in \u001b[0;36mSamplerQNN._backward\u001b[0;34m(self, input_data, weights)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 423\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m QiskitMachineLearningError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampler job failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/qiskit/primitives/primitive_job.py:51\u001b[0m, in \u001b[0;36mPrimitiveJob.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResultT:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_submitted()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date = '05_19_25_1'\n",
    "file_name = f'QNN/classification/result/{date}.csv'\n",
    "!mkdir QNN/classification/figure/$date\n",
    "\n",
    "main(file_name, date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac67dd-ef78-4aeb-9b89-223061137206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480044b-24d7-4e18-a5c4-b43a3c5a9fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
